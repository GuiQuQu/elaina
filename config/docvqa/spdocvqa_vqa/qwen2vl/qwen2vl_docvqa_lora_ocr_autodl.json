{
    "train_dataset": {
        "type": "docvqa_vqa_dataset",
        "dataset_path": "/root/autodl-tmp/SPDocVQA",
        "file_name": "train_v1.0_withQT",
        "preprocess_config": {
            "type": "docvqa_vqa_qwen2vl_preprocessor",
            "model_path": "/root/autodl-tmp/pretrain-model/Qwen2-VL-2B-Instruct",
            "max_seq_length": 2304,
            "use_ocr" : true,
            "max_layout_length": 704
        }
    },
    "val_dataset": {
        "type": "docvqa_vqa_dataset",
        "dataset_path": "/root/autodl-tmp/SPDocVQA",
        "file_name": "val_v1.0_withQT",
        "preprocess_config": {
            "type": "docvqa_vqa_qwen2vl_preprocessor",
            "model_path": "/root/autodl-tmp/pretrain-model/Qwen2-VL-2B-Instruct",
            "max_seq_length": 2304,
            "use_ocr" : true,
            "max_layout_length": 704
        }
    },
    "test_dataset": {
        "type": "docvqa_vqa_dataset",
        "dataset_path": "/root/autodl-tmp/SPDocVQA",
        "file_name": "val_v1.0_withQT",
        "preprocess_config": {
            "type": "docvqa_vqa_qwen2vl_preprocessor",
            "model_path": "/root/autodl-tmp/pretrain-model/Qwen2-VL-2B-Instruct",
            "max_seq_length": 2304,
            "use_ocr": true,
            "max_layout_length": 704
        }
    },
    "model": {
        "type": "qwen2vl_vqa_model",
        "model_path": "/root/autodl-tmp/pretrain-model/Qwen2-VL-2B-Instruct",
        "warp_qwen2vl_lora": 64,
        "freeze_vision_model": true,
        "freeze_llm_model": true,
        "model_dtype": "bf16"
    },
    "output_dir": "/root/autodl-tmp/outputs/SPDocVQA/qwen2vl_ocr_vqa_output",
    "data_collator": "qwen2vl_concat_collator",
    "tester": {
        "type": "custom_tester",
        "output_dir": "outputs/SPDocVQA/qwen2vl_ocr_vqa_output/test_result",
        "test_raw_model": true,
        "checkpoint_list": [
            "/root/autodl-tmp/outputs/SPDocVQA/qwen2vl_ocr_vqa_output/checkpoint-1000/pytorch_model.bin",
            "/root/autodl-tmp/outputs/SPDocVQA/qwen2vl_ocr_vqa_output/checkpoint-1000/pytorch_model.bin",
            "/root/autodl-tmp/outputs/SPDocVQA/qwen2vl_ocr_vqa_output/checkpoint-4000/pytorch_model.bin",
            "/root/autodl-tmp/outputs/SPDocVQA/qwen2vl_ocr_vqa_output/checkpoint-4932/pytorch_model.bin"
        ],
        "dataloader_config": {
            "max_steps": -1,
            "batch_size": 8,
            "num_workers": 4,
            "shuffle": false
        },
        "metrics": [
            {
                "type": "metrics.docvqa_metrics.anls_metrics.ANLSMetrics"
            }
        ]
    },
    "trainer": "hf_trainer",
    "training_args": {
        "do_train": true,
        "do_eval": false,
        "do_predict": false,
        "run_name": "train_run_v1",
        "overwrite_output_dir": true,
        "bf16": true,
        "gradient_checkpointing": true,
        "gradient_checkpointing_kwargs": {
            "use_reentrant": false
        },
        "remove_unused_columns": true,
        "num_train_epochs": 1,
        "per_device_train_batch_size": 8,
        "dataloader_pin_memory": true,
        "dataloader_prefetch_factor ": 1,
        "dataloader_num_workers": 4,
        "gradient_accumulation_steps": 1,
        "optim": "adamw_torch",
        "learning_rate": 1e-5,
        "max_grad_norm": 1.0,
        "weight_decay": 0.01,
        "lr_scheduler_type": "cosine",
        "warmup_ratio": 0.1,
        "evaluation_strategy": "no",
        "save_strategy": "steps",
        "save_steps": 1000,
        "save_safetensors": false,
        "save_total_limit": 5,
        "logging_strategy": "steps",
        "logging_steps": 10,
        "report_to": "tensorboard"
    }
}